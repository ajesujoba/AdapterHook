{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW9HlcqoZxur",
        "outputId": "84e3f118-c619-4b24-c725-31cb392bb212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: adapter-transformers in /usr/local/lib/python3.8/dist-packages (3.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->adapter-transformers) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->adapter-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->adapter-transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->adapter-transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->adapter-transformers) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.8/dist-packages (0.5.1)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchdata) (1.13.1+cu116)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from torchdata) (2.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchdata) (2.25.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.8/dist-packages (from torchdata) (1.26.14)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchdata) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n"
          ]
        }
      ],
      "source": [
        "#!pip install transformers\n",
        "!pip install -U adapter-transformers\n",
        "!pip install torchdata\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
        "from transformers.adapters.configuration import AdapterConfig\n",
        "import transformers.adapters.composition as ac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs48iK-BZ4qU",
        "outputId": "92836d87-cd05-405c-dc6b-32f148add22b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seet the hooks for the different modules, should work with or without adapters"
      ],
      "metadata": {
        "id": "jgBKoXyOdgam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Below I state the naming convention\n",
        "layer_dict = {\"layer_input_\":\"input into each layer, output of the previous layer (redundancy)\", \n",
        "              \"attention_output_\": \"output of the attention layer, before the upward and downward project using FFN\", \n",
        "              \"ffn_output_\": \"The feedforward downward projection before layer normalization\", \n",
        "              \"layer_output_\": \"this is the layer output which is the same as the hidden state\", \n",
        "              \"adapter_input_\": \"adapter layer input, this should be the output of ffn_output_ after layer normalization \",\n",
        "              \"adapter_output_\": \"adapter layer output\" \n",
        "              }\n",
        "for k, v in layer_dict.items():\n",
        "  print(k,':', v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM2joiXcdkr_",
        "outputId": "5e4f279f-ee96-4a8a-e410-db68e546e9b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer_input_ : input into each layer, output of the previous layer (redundancy)\n",
            "attention_output_ : output of the attention layer, before the upward and downward project using FFN\n",
            "ffn_output_ : The feedforward downward projection before layer normalization\n",
            "layer_output_ : this is the layer output which is the same as the hidden state\n",
            "adapter_input_ : adapter layer input, this should be the output of ffn_output_ after layer normalization \n",
            "adapter_output_ : adapter layer output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_hooks_roberta(model, lang=None):\n",
        "    \"\"\"\n",
        "    Only works on Roberta from HF\n",
        "    model: model object\n",
        "    lang: is the name of the adapter\n",
        "    \"\"\"\n",
        "    final_layer = model.config.num_hidden_layers\n",
        "\n",
        "    for attr in [\"activations_\"]:\n",
        "        if not hasattr(model, attr):\n",
        "            setattr(model, attr, {})\n",
        "\n",
        "    def get_activation(name):\n",
        "        def hook(module, input, output):\n",
        "            if \"input_embedding\" in name or \"layer_input\" in name:\n",
        "              num_tokens = list(input[0].size())[1]  # (batch, sequence, hidden_state)\n",
        "              model.activations_[name] = input[0].detach() #input[0].detach() #[:, num_tokens - 1].detach()\n",
        "            elif \"attention_output\" in name:\n",
        "              num_tokens = list(input[0].size())[1]  # (batch, sequence, hidden_state)\n",
        "              model.activations_[name] = input[0]\n",
        "            elif \"ffn_output\" in name or \"layer_output\" in name or \"ffn_output_norm\" in name:\n",
        "              num_tokens = list(output[0].size())[1]  # (batch, sequence, hidden_state)\n",
        "              model.activations_[name] = output[0].unsqueeze(dim=0)\n",
        "            elif \"adapter_input\" in name:\n",
        "              num_tokens = list(input[0].size())[1]  # (batch, sequence, hidden_state)\n",
        "              model.activations_[name] = input[0]\n",
        "            elif \"adapter_output\" in name:\n",
        "              num_tokens = list(output[0].size())[1]  # (batch, sequence, hidden_state)\n",
        "              model.activations_[name] = output[0].unsqueeze(dim=0)\n",
        "            elif \"adapter_output2\" in name:\n",
        "              num_tokens = list(output[0].size())[1]  # (batch, sequence, hidden_state)\n",
        "              model.activations_[name] = output[0].unsqueeze(dim=0)\n",
        "\n",
        "        return hook\n",
        "    \n",
        "    model.roberta.encoder.layer[0].attention.register_forward_hook(get_activation(\"input_embedding\"))\n",
        "    for i in range(final_layer):\n",
        "      model.roberta.encoder.layer[i].attention.register_forward_hook(get_activation(\"layer_input_\" + str(i))) # get the input to each attention module\n",
        "      model.roberta.encoder.layer[i].intermediate.register_forward_hook(get_activation(\"attention_output_\" + str(i))) # get the attention module output     \n",
        "      model.roberta.encoder.layer[i].output.dense.register_forward_hook(get_activation(\"ffn_output_\" + str(i))) # get output of the encoder FFN module before Layer normalization\n",
        "      model.roberta.encoder.layer[i].output.LayerNorm.register_forward_hook(get_activation(\"ffn_output_norm_\" + str(i))) # we need this input\n",
        "      model.roberta.encoder.layer[i].output.register_forward_hook(get_activation(\"layer_output_\" + str(i))) # redunancy because it is the hidden state also\n",
        "      if lang!=None:\n",
        "        # if we have a language adapter, get the adapter input and output\n",
        "        model.roberta.encoder.layer[i].output.adapters[lang].adapter_down.register_forward_hook(get_activation(\"adapter_input_\" + str(i)))\n",
        "        model.roberta.encoder.layer[i].output.adapters[lang].adapter_up.register_forward_hook(get_activation(\"adapter_output_\" + str(i)))\n",
        "        #model.get_adapter(lang)[i]['output_adapter'].register_forward_hook(get_activation(\"adapter_output2\"))\n",
        "        #model.get_adapter(lang)[i]['output_adapter'].register_forward_hook(get_activation(\"adapter_input2\"))\n",
        "        #model.get_adapter(lang)[i]['output_adapter'].adapter_up.register_forward_hook(get_activation(\"adapter_output3\"))\n",
        "    "
      ],
      "metadata": {
        "id": "3bRTowt8aIf2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hidden_states(model, tokenizer, tokens, TOP_K=1):\n",
        "    \"\"\"\n",
        "    get the hidden states for all the tokens and the unknown tokens that were masked from all the modules set in the hook above\n",
        "    input\n",
        "      model: model \n",
        "      tokenizer: tokenizer\n",
        "      tokens: input sentence already tokenized\n",
        "\n",
        "    returns\n",
        "      sent_to_hidden_states: intermediate hidden states gotten from hook\n",
        "      unknown_hiddens: hidden states for only the masked tokens gotten from hook (subset of sent_to_hidden_states)\n",
        "      hidden_states: the original encoder hidden states (each layer hidden states)\n",
        "\n",
        "    \"\"\"\n",
        "    HIDDEN_SIZE = model.config.hidden_size\n",
        "    masked_position = (tokens['input_ids'].squeeze() == tokenizer.mask_token_id).nonzero()\n",
        "    masked_position = masked_position.squeeze()\n",
        "    #print(\"Masked positions = \", masked_position)\n",
        "    layer_residual_preds = []\n",
        "    intermed_residual_preds = []\n",
        "    output = model(**tokens, output_hidden_states=True)\n",
        "    hidden_states = output['hidden_states']\n",
        "    #print(hidden[-1])\n",
        "    sent_to_hidden_states = {}\n",
        "    unknow_hiddens = {}\n",
        "    for layer in model.activations_.keys():\n",
        "      if 'input_embedding' in layer or \"layer_input\" in layer or \"attention_output\" in layer or \"ffn_output\" in layer or \"layer_output\" in layer or \"adapter_input\" in layer or \"adapter_output\" in layer or \"ffn_output_norm\" in layer:\n",
        "        sent_to_hidden_states[layer] = model.activations_[layer]\n",
        "        unknow_hiddens[layer] = model.activations_[layer][:, masked_position]\n",
        "    return sent_to_hidden_states, unknow_hiddens, hidden_states"
      ],
      "metadata": {
        "id": "B2r9j8vNaIpo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model and language adapter"
      ],
      "metadata": {
        "id": "ZztGzCBNeUg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadmodel(name=\"xlm-roberta-base\", lang=\"en\"):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(name) \n",
        "  config = AutoConfig.from_pretrained(name)\n",
        "  model = AutoModelForMaskedLM.from_pretrained(name, config=config) \n",
        "  lang_adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=8)\n",
        "  model.load_adapter(lang, config=lang_adapter_config, model_name = 'xlm-roberta-base')\n",
        "  model.set_active_adapters(ac.Stack(lang))\n",
        "  return model, tokenizer"
      ],
      "metadata": {
        "id": "vNnMg2W5ejKR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test with a sample input sentence in English"
      ],
      "metadata": {
        "id": "xSVH84URfMWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelname = \"xlm-roberta-base\"\n",
        "lang = \"en\"\n",
        "model, tokenizer = loadmodel(name=modelname, lang=lang)\n",
        "set_hooks_roberta(model,lang=lang)\n",
        "sampleinput =  \"Tom has fully recovered from the illness.\"\n",
        "sentence = \"Tom has fully ___ from the illness.\"\n",
        "sentence = sentence.replace(\"___\",\"<mask>\")\n",
        "\n",
        "# key = (sentence, idx)\n",
        "tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "tokens_to_sent = tokenizer.tokenize(sentence)\n",
        "sent_to_hidden_states, unknow_hiddens, encoderhidden = get_hidden_states(model,tokenizer, tokens, TOP_K=30)"
      ],
      "metadata": {
        "id": "9Zksq9EAZ4yK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The issue I am talking about is below, using the first encoder layer as an example, below I print out the hidden state from the first encoder layer, the output of the transformer hidden state (which should be the input to the adapter module), and the adapter module output,"
      ],
      "metadata": {
        "id": "vyUlmLvkgZ1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the output of the first encoder layer , encoderhidden[0] is the embedding layer\n",
        "print(encoderhidden[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2b1Guc_gdgO",
        "outputId": "3592231c-c941-4d41-c827-74f68da84754"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0481, -0.0368, -0.0395,  ..., -0.0945, -0.0946, -0.2492],\n",
            "         [-0.2660, -0.2258,  0.4876,  ...,  0.2622,  0.0763,  1.1179],\n",
            "         [-0.2593,  0.1817,  0.2864,  ...,  1.0401, -0.8326,  0.0481],\n",
            "         ...,\n",
            "         [ 0.2902,  0.1266,  0.1916,  ...,  0.4514,  0.4580, -0.2023],\n",
            "         [ 0.1633,  0.1977,  0.2310,  ..., -0.3881,  0.0034,  0.1870],\n",
            "         [ 0.0400,  0.3386,  0.2164,  ...,  0.5326, -0.1517,  0.3164]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the output of the transformer layer, this is suppose to be the input into the language adapter\n",
        "print(sent_to_hidden_states['ffn_output_norm_0']) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xvYKnVbaC6l",
        "outputId": "e272903c-942e-4754-ea6c-f72747c4252b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0481, -0.0368, -0.0395,  ..., -0.0945, -0.0946, -0.2492],\n",
            "         [-0.2660, -0.2258,  0.4876,  ...,  0.2622,  0.0763,  1.1179],\n",
            "         [-0.2593,  0.1817,  0.2864,  ...,  1.0401, -0.8326,  0.0481],\n",
            "         ...,\n",
            "         [ 0.2902,  0.1266,  0.1916,  ...,  0.4514,  0.4580, -0.2023],\n",
            "         [ 0.1633,  0.1977,  0.2310,  ..., -0.3881,  0.0034,  0.1870],\n",
            "         [ 0.0400,  0.3386,  0.2164,  ...,  0.5326, -0.1517,  0.3164]]],\n",
            "       grad_fn=<UnsqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The adapter output\")\n",
        "print(sent_to_hidden_states['adapter_output_0'])"
      ],
      "metadata": {
        "id": "hcx5lWiXg5va",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7a20eb-d844-4584-e0db-3b486bd7e197"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The adapter output\n",
            "tensor([[[ 2.9568e-01, -2.0675e-01,  1.4939e-01,  ..., -1.6568e-02,\n",
            "           1.5798e-01,  6.3218e-02],\n",
            "         [-1.1226e-02, -2.7732e-02,  1.1889e-03,  ...,  2.3751e-03,\n",
            "           1.1180e-02,  1.5625e-03],\n",
            "         [ 6.1715e-02,  1.1988e-02, -1.4756e-01,  ...,  1.2963e-01,\n",
            "          -5.8567e-02, -1.4035e-02],\n",
            "         ...,\n",
            "         [ 4.5658e-03,  8.2757e-02, -4.9524e-02,  ...,  4.1237e-02,\n",
            "          -8.3173e-03, -1.3748e-04],\n",
            "         [ 4.4017e-02,  6.8796e-02, -3.6980e-02,  ..., -1.0728e-01,\n",
            "           6.8577e-02,  5.9167e-02],\n",
            "         [-3.5689e-03,  5.2253e-02, -1.4614e-01,  ...,  5.5502e-02,\n",
            "          -4.6489e-02,  5.3878e-02]]], grad_fn=<UnsqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take logit over the output hidden state, to the top-k output"
      ],
      "metadata": {
        "id": "XpoacISEi63N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model.lm_head(unknow_hiddens['layer_output_11'])\n",
        "probs = F.softmax(logits)\n",
        "probs_ = []\n",
        "TOP_K = 10\n",
        "for index, prob in enumerate(probs.squeeze()):\n",
        "      probs_.append((index, prob))\n",
        "top_k = sorted(probs_, key=lambda x: x[1], reverse=True)[:TOP_K]\n",
        "print(top_k)\n",
        "top_k = [(t[1].item(), t[0]) for t in top_k]\n",
        "print([tokenizer.decode(y).strip() for x, y in top_k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhtcClv_3rK7",
        "outputId": "cc7caa40-fb96-4278-a93c-c716eccc3dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-59873a33aec0>:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs = F.softmax(logits)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i0FoPmYljQ3_"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}